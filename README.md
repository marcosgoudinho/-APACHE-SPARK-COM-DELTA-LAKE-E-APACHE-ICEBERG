# ğŸš€ Apache Spark com Delta Lake e Apache Iceberg

Projeto desenvolvido como parte da disciplina de Engenharia de Dados com o objetivo de demonstrar a criaÃ§Ã£o de um ambiente PySpark integrado com Delta Lake e Apache Iceberg, utilizando o gerenciador de pacotes Poetry.

---

## ğŸ“„ DescriÃ§Ã£o

Este repositÃ³rio contÃ©m:
- ConfiguraÃ§Ã£o de ambiente usando **Poetry**
- Notebooks com exemplos prÃ¡ticos de manipulaÃ§Ã£o de dados com **Delta Lake** e **Apache Iceberg**
- DocumentaÃ§Ã£o gerada com **MKDocs**
- Modelo ER, comandos DDL e exemplos de uso real

---

## âš ï¸ Aviso

Este projeto tem fins educacionais e pode exigir ajustes dependendo do seu sistema operacional e versÃ£o do Python. Recomendado uso em mÃ¡quinas com sistema baseado em Unix (Linux/Mac) ou com Windows configurado com Python corretamente e PATH ajustado.

---

## âœ… PrÃ©-requisitos

- **Python 3.10+**
- **Poetry** instalado ([guia oficial](https://python-poetry.org/docs/#installation))
- **Java JDK 8 ou superior**
- **Jupyter Lab**

DependÃªncias principais:

```toml
pyspark = "^3.5.0"
delta-spark = "^3.1.0"
jupyterlab = "^4.0.0"
pyarrow = "^15.0.0"
mkdocs = "^1.5.0"
mkdocs-material = "^9.4.0"
```

---

## ğŸ› ï¸ InstalaÃ§Ã£o

### 1. Clone o projeto

```bash
git clone https://github.com/marcosgoudinho/-APACHE-SPARK-COM-DELTA-LAKE-E-APACHE-ICEBERG.git
cd spark-delta-iceberg
```

### 2. Instale as dependÃªncias

```bash
poetry install
```

### 3. Ative o ambiente virtual

```bash
poetry shell
```

### 4. Rode o Jupyter Lab

```bash
jupyter lab
```

> **Importante:** Caso o kernel Jupyter nÃ£o esteja disponÃ­vel, execute:
>
> ```bash
> python -m ipykernel install --user --name=spark-lake-project
> ```

---

## ğŸ§ª How to: Como usar o projeto

### âš¡ SparkSession com Delta e Iceberg

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SparkLake") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog") \
    .config("spark.sql.catalog.spark_catalog.type", "hive") \
    .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.local.type", "hadoop") \
    .config("spark.sql.catalog.local.warehouse", "./warehouse") \
    .getOrCreate()
```

### ğŸ§Š Iceberg: Exemplo de uso

```python
spark.sql("CREATE TABLE local.db.iceberg_table (id BIGINT) USING ICEBERG")
spark.sql("INSERT INTO local.db.iceberg_table VALUES (1), (2), (3)")
```

### ğŸ”º Delta Lake: Exemplo de uso

```python
data = spark.range(0, 5)
data.write.format("delta").mode("overwrite").save("warehouse/delta-table")
```

---

## ğŸ§ª Testes

> Este projeto nÃ£o possui testes automatizados. Recomendado executar os notebooks e validar outputs esperados no prÃ³prio ambiente Jupyter.

---

## ğŸ“š DocumentaÃ§Ã£o

Acesse a documentaÃ§Ã£o completa (gerada com MKDocs):

ğŸ”— [DocumentaÃ§Ã£o do Projeto (localhost)](http://127.0.0.1:8000)

Para rodar localmente:

```bash
mkdocs serve
```

---
